import streamlit as st
import requests
import os

# -----------------------------
# CONFIGURATION DE LA PAGE
# -----------------------------
st.set_page_config(
    page_title="Tuteur √âducatif Personnalis√© (LLM)",
    page_icon="üéì",
    layout="centered"
)

# Style CSS personnalis√©
st.markdown("""
<style>
    .main {
        padding: 2rem;
    }
    .stTextArea textarea {
        font-size: 16px;
    }
    .stButton button {
        background-color: #4CAF50;
        color: white;
        font-weight: bold;
        padding: 0.5rem 2rem;
        border-radius: 5px;
        border: none;
    }
    .stButton button:hover {
        background-color: #45a049;
    }
</style>
""", unsafe_allow_html=True)

st.title("üéì Tuteur √âducatif Personnalis√©")
st.markdown("""
### Bienvenue dans votre assistant p√©dagogique intelligent
Ce tuteur utilise un **Large Language Model (LLM)** pour accompagner 
les √©tudiants de **Licence 3 Informatique** de mani√®re personnalis√©e.
""")

# -----------------------------
# SIDEBAR POUR LES INFORMATIONS
# -----------------------------
with st.sidebar:
    st.header("‚ÑπÔ∏è Informations")
    st.write("""
    **Fonctionnalit√©s :**
    - Adaptation au niveau
    - Explications p√©dagogiques
    - Exemples concrets
    - V√©rification de compr√©hension
    
    **Mati√®res disponibles :**
    1. Programmation Python
    2. Algorithmique
    """)
    
    st.header("üîë Configuration API")
    api_source = st.radio(
        "Source de l'API :",
        ["Hugging Face", "OpenAI (√† venir)"]
    )
    
    if api_source == "Hugging Face":
        st.info("Utilise l'API Hugging Face avec Mistral 7B")
    else:
        st.warning("Option en d√©veloppement")

# -----------------------------
# PARAM√àTRES UTILISATEUR
# -----------------------------
col1, col2 = st.columns(2)

with col1:
    matiere = st.selectbox(
        "üìò Choisissez la mati√®re :",
        ["Programmation Python", "Algorithmique et structures de donn√©es"],
        help="S√©lectionnez la mati√®re que vous souhaitez √©tudier"
    )

with col2:
    niveau = st.selectbox(
        "üéØ Choisissez votre niveau :",
        ["D√©butant", "Interm√©diaire", "Avanc√©"],
        help="Cela permet d'adapter le niveau d'explication"
    )

st.markdown("---")

question = st.text_area(
    "‚úèÔ∏è Posez votre question :",
    placeholder="Ex : Explique-moi les boucles en Python\nOu : Quelle est la diff√©rence entre une liste et un tuple ?",
    height=150
)

# -----------------------------
# CL√â API HUGGING FACE
# -----------------------------
# Pour Vercel, utilisez les variables d'environnement
HF_API_TOKEN = st.secrets.get("HF_API_TOKEN", os.getenv("HF_API_TOKEN"))

API_URL = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2"
headers = {
    "Authorization": f"Bearer {HF_API_TOKEN}"
} if HF_API_TOKEN else {}

# -----------------------------
# FONCTION D'APPEL AU LLM AM√âLIOR√âE
# -----------------------------
def appeler_llm(prompt):
    try:
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": 800,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "return_full_text": False
            },
            "options": {
                "use_cache": True,
                "wait_for_model": True
            }
        }
        
        response = requests.post(API_URL, headers=headers, json=payload, timeout=60)
        
        if response.status_code == 200:
            return response.json()
        elif response.status_code == 503:
            return {"error": "Le mod√®le est en cours de chargement, veuillez r√©essayer dans 30 secondes"}
        else:
            return {"error": f"Erreur API: {response.status_code}"}
            
    except requests.exceptions.Timeout:
        return {"error": "Timeout - Le serveur met trop de temps √† r√©pondre"}
    except Exception as e:
        return {"error": f"Exception: {str(e)}"}

# -----------------------------
# BOUTON DE G√âN√âRATION
# -----------------------------
col1, col2, col3 = st.columns([1, 2, 1])
with col2:
    generer = st.button("üì§ Obtenir l'explication", use_container_width=True)

if generer:
    if not HF_API_TOKEN:
        st.error("""
        ‚ùå Cl√© API Hugging Face manquante.
        
        **Pour tester localement :**
        1. Cr√©ez un fichier `.streamlit/secrets.toml`
        2. Ajoutez : `HF_API_TOKEN = "votre_token_ici"`
        3. Ou d√©finissez la variable d'environnement
        
        **Pour Vercel :**
        1. Allez dans Project Settings > Environment Variables
        2. Ajoutez HF_API_TOKEN avec votre token
        """)
    elif question.strip() == "":
        st.warning("‚ö†Ô∏è Veuillez entrer une question.")
    else:
        with st.spinner("‚è≥ Le tuteur r√©fl√©chit √† la meilleure explication pour vous..."):
            # Construction du prompt p√©dagogique
            prompt = f"""
Tu es un tuteur universitaire expert en informatique. Tu r√©ponds √† un √©tudiant de Licence 3.

CONTEXTE:
- Mati√®re: {matiere}
- Niveau de l'√©tudiant: {niveau}
- Question: {question}

INSTRUCTIONS P√âDAGOGIQUES:
1. ADAPTE ton langage au niveau ({niveau})
   - D√©butant: termes simples, m√©taphores
   - Interm√©diaire: concepts techniques avec explications
   - Avanc√©: d√©tails techniques, bonnes pratiques

2. STRUCTURE ta r√©ponse:
   a) Introduction claire du concept
   b) Explication progressive
   c) Exemple concret en code si applicable
   d) Points cl√©s √† retenir
   e) Question de v√©rification de compr√©hension

3. TON STYLE:
   - Encourageant et positif
   - P√©dagogique mais pas condescendant
   - Pr√©cis techniquement
   - Utilise des analogies si utile

4. √Ä LA FIN, pose UNE question simple pour v√©rifier que l'√©tudiant a compris.

COMMENCE TA R√âPONSE DIRECTEMENT:
"""

            resultat = appeler_llm(prompt)

        st.markdown("---")
        
        if isinstance(resultat, list):
            if "generated_text" in resultat[0]:
                st.success("‚úÖ R√©ponse du tuteur")
                st.markdown("### üìù Explication :")
                st.markdown(resultat[0]["generated_text"])
                
                # Section feedback
                st.markdown("---")
                st.subheader("üìä √âvaluez cette r√©ponse")
                col1, col2, col3 = st.columns(3)
                with col1:
                    if st.button("üëç Compris"):
                        st.balloons()
                        st.success("Super ! Continuons d'apprendre.")
                with col2:
                    if st.button("ü§î Pas clair"):
                        st.info("Essayez de reformuler votre question plus simplement.")
                with col3:
                    if st.button("üîÑ Nouvelle question"):
                        st.experimental_rerun()
            else:
                st.error("Format de r√©ponse inattendu de l'API")
        elif isinstance(resultat, dict) and "error" in resultat:
            st.error(f"‚ö†Ô∏è {resultat['error']}")
            st.info("üí° Conseil : Essayez avec une question plus simple ou r√©essayez plus tard.")
        else:
            st.error("R√©ponse inattendue de l'API")

# -----------------------------
# FOOTER
# -----------------------------
st.markdown("---")
st.caption("""
üéì **Tuteur √âducatif Personnalis√©** - Projet LLM 
| D√©ploy√© sur Vercel | Licence 3 Informatique
""")